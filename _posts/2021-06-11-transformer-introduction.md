---
title: "Transformer : Transformer in Computer Vision 논문 리뷰를 시작하겠습니다."
categories:
  - Transformer
tags:
  - Transformer
  - NLP
  - Vision
  - CVPR 2021
---

안녕하세요. 논문을 읽고 리뷰하고 그 내용을 공유하고자 블로그를 새롭게 작성하게 되었습니다. 
제일 처음에 공부해볼 내용은 Transformer 입니다.

아래의 순서대로 논문을 읽고 리뷰할 생각입니다.

   * Attention is all you need (2017) 
   * Transformers in Vision: A Survey (2021) 

먼저 Transformer Base 논문과 Survey 논문을 읽고난 후

   *  Transformer Meets Tracker Exploiting Temporal Context for Robust Visual Tracking(CVPR 2021) 
   *  Variational Transformer Networks for Layout(CVPR 2021) 
   *  LoFTR Detector-Free Local Feature Matching with Transformers(CVPR 2021) 
   *  Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers(CVPR 2021) 
   *  Thinking Fast and Slow Efficient Text-to-Visual Retrieval with Transformers(CVPR 2021) 
   *  Transformer Tracking(CVPR 2021) 
   *  MIST Multiple Instance Spatial Transformer Network(CVPR 2021) 
   *  Multimodal Motion Prediction with Stacked Transformers(CVPR 2021) 
   *  Revamping cross-modal recipe retrieval with hierarchical Transformers and self-supervised learning(CVPR 2021) 
   *  Pre-Trained Image Processing Transformer(CVPR 2021) 
   *  End-to-End Video Instance Segmentation with Transformers(CVPR 2021) 
   *  UP-DETR Unsupervised Pre-training for Object Detection with Transformers(CVPR 2021) 
   *  End-to-End Human Object Interaction Detection with HOI Transformer(CVPR 2021) 
   *  Transformer Interpretability Beyond Attention Visualization(CVPR 2021) 

CVPR 2021에 나온 Transformer 최신 논문들을 리뷰할 생각입니다.
부족한 점이 보일때마다 댓글로 말씀해주시면 감사하겠습니다.